{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6798760c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import random\n",
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "from PIL import Image\n",
    "import h5py\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms as T\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "from torch.optim.lr_scheduler import OneCycleLR\n",
    "\n",
    "from torchmetrics.image import StructuralSimilarityIndexMeasure\n",
    "from torchmetrics.functional import peak_signal_noise_ratio as psnr\n",
    "from torchmetrics.image.lpip import LearnedPerceptualImagePatchSimilarity as LPIPS\n",
    "from torchvision.models import vgg16, VGG16_Weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3f9d20ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ────────────────────────────────────────────────────────────────────────────────\n",
    "# CONFIGURATION\n",
    "# ────────────────────────────────────────────────────────────────────────────────\n",
    "class Config:\n",
    "    DATA_PATH    = \"/Users/imamahasan/MyData/Code/lightcyclegan_ld_v3\"  # <-- your root\n",
    "    SAVE_DIR     = \"./checkpoints\"\n",
    "    BATCH_SIZE   = 4\n",
    "    EPOCHS       = 100\n",
    "    LR           = 2e-4\n",
    "    WEIGHT_DECAY = 1e-4\n",
    "    PATIENCE     = 10\n",
    "\n",
    "    # Loss weights\n",
    "    L_ADV   = 1.0\n",
    "    L_PERP  = 1.0\n",
    "    L_SSIM  = 5.0\n",
    "    L_CYCLE = 10.0\n",
    "    L_EDGE  = 0.2\n",
    "    L_FFL   = 0.5\n",
    "\n",
    "    # Dimensions (original LoDoPaB sizes)\n",
    "    SINO_SHAPE = (1000, 513)\n",
    "    IMG_SHAPE  = (362, 362)\n",
    "\n",
    "    DEVICE      = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "    USE_AMP     = False if DEVICE.type == \"mps\" else True\n",
    "    MAX_LR      = LR\n",
    "    NUM_WORKERS = 0\n",
    "\n",
    "os.makedirs(Config.SAVE_DIR, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "84292122",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ────────────────────────────────────────────────────────────────────────────────\n",
    "# DATASET\n",
    "# ────────────────────────────────────────────────────────────────────────────────\n",
    "class LoDoPaBSinogramDataset(Dataset):\n",
    "    def __init__(self, sino_dir, gt_dir):\n",
    "        self.sino_files = sorted(\n",
    "            os.path.join(sino_dir, f)\n",
    "            for f in os.listdir(sino_dir) if f.endswith(\".hdf5\")\n",
    "        )\n",
    "        self.gt_files = sorted(\n",
    "            os.path.join(gt_dir, f)\n",
    "            for f in os.listdir(gt_dir) if f.endswith(\".hdf5\")\n",
    "        )\n",
    "        assert self.sino_files, f\"No sinogram files in {sino_dir}\"\n",
    "        assert self.gt_files,   f\"No GT files in {gt_dir}\"\n",
    "        assert len(self.sino_files) == len(self.gt_files), \"File count mismatch\"\n",
    "\n",
    "        self.indices = []\n",
    "        for idx, (sf, gf) in enumerate(zip(self.sino_files, self.gt_files)):\n",
    "            with h5py.File(sf, 'r') as fs, h5py.File(gf, 'r') as fg:\n",
    "                n = min(len(fs['data']), len(fg['data']))\n",
    "            assert n > 0, f\"No slices in {sf}\"\n",
    "            self.indices += [(idx, i) for i in range(n)]\n",
    "        print(f\"Loaded {len(self.indices)} slices from {len(self.sino_files)} files\")\n",
    "\n",
    "        self.tf_sino = T.Compose([\n",
    "            T.Resize(Config.SINO_SHAPE),\n",
    "            T.ToTensor()\n",
    "        ])\n",
    "        self.tf_img = T.Compose([\n",
    "            T.Resize(Config.IMG_SHAPE),\n",
    "            T.ToTensor()\n",
    "        ])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.indices)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        file_idx, slice_idx = self.indices[idx]\n",
    "        sf, gf = self.sino_files[file_idx], self.gt_files[file_idx]\n",
    "        with h5py.File(sf,'r') as fs:\n",
    "            sino = fs['data'][slice_idx].astype(np.float32)\n",
    "        with h5py.File(gf,'r') as fg:\n",
    "            img  = fg['data'][slice_idx].astype(np.float32)\n",
    "\n",
    "        # normalize\n",
    "        sino = (sino - sino.min())/(sino.max()-sino.min()+1e-8)\n",
    "        img  = (img  - img.min())/(img.max()-img.min()+1e-8)\n",
    "\n",
    "        # ToTensor on a grayscale PIL gives shape [1,H,W]\n",
    "        sino_t = self.tf_sino(Image.fromarray((sino*255).astype(np.uint8)))\n",
    "        img_t  = self.tf_img (Image.fromarray((img *255).astype(np.uint8)))\n",
    "\n",
    "        # **do not** unsqueeze here!\n",
    "        return sino_t, img_t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "34aab53d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ────────────────────────────────────────────────────────────────────────────────\n",
    "# MODEL COMPONENTS\n",
    "# ────────────────────────────────────────────────────────────────────────────────\n",
    "class GhostModule(nn.Module):\n",
    "    def __init__(self, inp, oup, ratio=2, primary_kernel=1, dw_kernels=(3,5)):\n",
    "        super().__init__()\n",
    "        self.init_c  = math.ceil(oup/ratio)\n",
    "        self.cheap_c = self.init_c*(ratio-1)\n",
    "        self.primary = nn.Conv2d(inp, self.init_c, primary_kernel,\n",
    "                                 padding=primary_kernel//2, bias=False)\n",
    "        self.dw_convs = nn.ModuleList([\n",
    "            nn.Conv2d(self.init_c, self.init_c, k,\n",
    "                      padding=k//2, groups=self.init_c, bias=False)\n",
    "            for k in dw_kernels\n",
    "        ])\n",
    "        self.bn = nn.BatchNorm2d(oup)\n",
    "    def forward(self, x):\n",
    "        x1 = self.primary(x)\n",
    "        x2 = torch.cat([dw(x1) for dw in self.dw_convs], dim=1)[:, :self.cheap_c]\n",
    "        out = torch.cat([x1, x2], dim=1)[:, :self.bn.num_features]\n",
    "        return self.bn(out)\n",
    "\n",
    "class CondConv(nn.Module):\n",
    "    def __init__(self, inp, oup, exp=2):\n",
    "        super().__init__()\n",
    "        self.experts = nn.ModuleList(\n",
    "            nn.Conv2d(inp, oup, 3, padding=1, bias=False) for _ in range(exp)\n",
    "        )\n",
    "        self.attn = nn.Sequential(\n",
    "            nn.AdaptiveAvgPool2d(1),\n",
    "            nn.Conv2d(inp, exp, 1),\n",
    "            nn.Softmax(dim=1)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        w = self.attn(x)\n",
    "        return sum(w[:, i:i+1] * conv(x) for i, conv in enumerate(self.experts))\n",
    "\n",
    "class ECABlock(nn.Module):\n",
    "    def __init__(self, c, gamma=2, b=1):\n",
    "        super().__init__()\n",
    "        t = int(abs((math.log2(c) + b)/gamma))\n",
    "        k = t if t%2 else t+1\n",
    "        self.conv1d = nn.Conv1d(1,1,k,padding=k//2,bias=False)\n",
    "    def forward(self, x):\n",
    "        y = x.mean(dim=(2,3)).unsqueeze(1)\n",
    "        y = self.conv1d(y)\n",
    "        w = y.sigmoid().squeeze(1).unsqueeze(-1).unsqueeze(-1)\n",
    "        return x * w\n",
    "\n",
    "class MixStyle(nn.Module):\n",
    "    def __init__(self, p=0.5, alpha=0.1):\n",
    "        super().__init__()\n",
    "        self.p = p\n",
    "        self.beta = torch.distributions.Beta(alpha, alpha)\n",
    "    def forward(self, x):\n",
    "        if not self.training or random.random() > self.p:\n",
    "            return x\n",
    "        B,C,H,W = x.size()\n",
    "        mu = x.mean([2,3], keepdim=True)\n",
    "        sig= x.var([2,3], keepdim=True).sqrt().add(1e-6)\n",
    "        xn = (x - mu)/sig\n",
    "        perm = torch.randperm(B)\n",
    "        mu2,sig2 = mu[perm], sig[perm]\n",
    "        l = self.beta.sample((B,1,1,1)).to(x.device)\n",
    "        mu_m  = mu*l + mu2*(1-l)\n",
    "        sig_m = sig*l + sig2*(1-l)\n",
    "        return xn*sig_m + mu_m\n",
    "\n",
    "class SinogramEncoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Conv2d(1,32,3,padding=1), nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(32,64,3,padding=1,stride=2), nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(64,128,3,padding=1,stride=2), nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(128,64,3,padding=1),    nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(64,32,3,padding=1),     nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(32,1,3,padding=1),\n",
    "            # Upsample back to target image size\n",
    "            nn.Upsample(size=Config.IMG_SHAPE, mode='bilinear', align_corners=False)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.ms     = MixStyle()\n",
    "        self.init   = GhostModule(1,16)\n",
    "        # self.d1,a1  = CondConv(16,32,exp=2), ECABlock(32)\n",
    "        # self.d2,a2  = CondConv(32,64,exp=3), ECABlock(64)\n",
    "        self.d1, self.a1 = CondConv(16,32,exp=2), ECABlock(32)\n",
    "        self.d2, self.a2 = CondConv(32,64,exp=3), ECABlock(64)\n",
    "        self.pool   = nn.AvgPool2d(2)\n",
    "        self.res    = nn.Sequential(*[GhostModule(64,64) for _ in range(2)])\n",
    "        self.bridge = ECABlock(64)\n",
    "        # <<-- FIXED: use keyword args for Upsample\n",
    "        self.u1     = nn.Sequential(\n",
    "            nn.Upsample(scale_factor=2, mode='bilinear', align_corners=False),\n",
    "            GhostModule(64,64)\n",
    "        )\n",
    "        self.u2     = nn.Sequential(\n",
    "            nn.Upsample(scale_factor=2, mode='bilinear', align_corners=False),\n",
    "            GhostModule(64,32)\n",
    "        )\n",
    "        self.final  = nn.Conv2d(32,1,3,padding=1)\n",
    "        self.output_resize = nn.Upsample(size=Config.IMG_SHAPE, mode='bilinear', align_corners=False)\n",
    "        self.act    = nn.ReLU(inplace=True)\n",
    "    def _crop_to_match(self, tensor1, tensor2):\n",
    "        \"\"\"Crop tensor1 to match the spatial dimensions of tensor2.\"\"\"\n",
    "        _, _, h1, w1 = tensor1.size()\n",
    "        _, _, h2, w2 = tensor2.size()\n",
    "        h = min(h1, h2)\n",
    "        w = min(w1, w2)\n",
    "        return tensor1[:, :, :h, :w]\n",
    "\n",
    "    def forward(self, x):\n",
    "        x   = self.ms(x)\n",
    "        e0  = self.act(self.init(x))\n",
    "        e1  = self.act(self.a1(self.d1(e0))); e1p = self.pool(e1)\n",
    "        e2  = self.act(self.a2(self.d2(e1p))); e2p = self.pool(e2)\n",
    "        r   = self.res(e2p); b = self.bridge(r)\n",
    "        u1_out = self.u1(b)\n",
    "        e2_cropped = self._crop_to_match(e2, u1_out)\n",
    "        d1  = self.act(u1_out + e2_cropped)\n",
    "        u2_out = self.u2(d1)\n",
    "        e1_cropped = self._crop_to_match(e1, u2_out)\n",
    "        d2  = self.act(u2_out + e1_cropped)\n",
    "        out = torch.tanh(self.final(d2))\n",
    "        return self.output_resize(out)\n",
    "\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            GhostModule(1,8),  nn.AvgPool2d(2),\n",
    "            GhostModule(8,16), nn.AvgPool2d(2),\n",
    "            GhostModule(16,32), nn.Conv2d(32,1,3,padding=1)\n",
    "        )\n",
    "    def forward(self,x): return self.net(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "35c2f0b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ────────────────────────────────────────────────────────────────────────────────\n",
    "# LOSSES\n",
    "# ────────────────────────────────────────────────────────────────────────────────\n",
    "def edge_loss(pred, tgt):\n",
    "    k = torch.tensor([[1,2,1],[0,0,0],[-1,-2,-1]],\n",
    "                     device=pred.device,dtype=pred.dtype).view(1,1,3,3)\n",
    "    return F.l1_loss(F.conv2d(pred,k,padding=1),\n",
    "                     F.conv2d(tgt,k,padding=1))\n",
    "\n",
    "def focal_frequency_loss(pred,tgt,chi=1.0):\n",
    "    dev = pred.device\n",
    "    if dev.type==\"mps\":  # CPU fallback\n",
    "        p,t = pred.float().cpu(), tgt.float().cpu()\n",
    "        Yp = torch.fft.rfft2(p,norm='ortho'); Yt = torch.fft.rfft2(t,norm='ortho')\n",
    "        wf = torch.log(torch.abs(Yt)**2+1e-8)**chi\n",
    "        return F.l1_loss(Yp*wf,Yt*wf).to(dev)\n",
    "    Yp = torch.fft.rfft2(pred,norm='ortho'); Yt = torch.fft.rfft2(tgt,norm='ortho')\n",
    "    wf = torch.log(torch.abs(Yt)**2+1e-8)**chi\n",
    "    return F.l1_loss(Yp*wf,Yt*wf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adc23edc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: mps\n",
      "Loaded 35820 slices from 280 files\n",
      "Loaded 3522 slices from 28 files\n",
      "Device: mps  AMP: False\n",
      "Train samples: 35820, Val samples: 3522\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/qk/szcdgwl97f98j7b69xpwh2200000gn/T/ipykernel_5858/497922316.py:33: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler   = GradScaler(enabled=Config.USE_AMP)\n",
      "/opt/homebrew/Caskroom/miniconda/base/envs/myenv/lib/python3.11/site-packages/torchmetrics/functional/image/lpips.py:326: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  self.load_state_dict(torch.load(model_path, map_location=\"cpu\"), strict=False)\n",
      "Epoch 1:   0%|          | 0/8955 [00:00<?, ?it/s]/var/folders/qk/szcdgwl97f98j7b69xpwh2200000gn/T/ipykernel_5858/497922316.py:51: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=Config.USE_AMP):\n",
      "/var/folders/qk/szcdgwl97f98j7b69xpwh2200000gn/T/ipykernel_5858/497922316.py:67: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=Config.USE_AMP):\n",
      "/opt/homebrew/Caskroom/miniconda/base/envs/myenv/lib/python3.11/site-packages/torchmetrics/utilities/prints.py:70: FutureWarning: Importing `peak_signal_noise_ratio` from `torchmetrics.functional` was deprecated and will be removed in 2.0. Import `peak_signal_noise_ratio` from `torchmetrics.image` instead.\n",
      "  _future_warning(\n",
      "Epoch 1:   4%|▍         | 383/8955 [02:26<50:23,  2.84it/s] "
     ]
    }
   ],
   "source": [
    "# ────────────────────────────────────────────────────────────────────────────────\n",
    "# TRAINING\n",
    "# ────────────────────────────────────────────────────────────────────────────────\n",
    "def train_and_evaluate():\n",
    "    train_ds = LoDoPaBSinogramDataset(\n",
    "        os.path.join(Config.DATA_PATH,\"observation_train\"),\n",
    "        os.path.join(Config.DATA_PATH,\"ground_truth_train\")\n",
    "    )\n",
    "    val_ds = LoDoPaBSinogramDataset(\n",
    "        os.path.join(Config.DATA_PATH,\"observation_validation\"),\n",
    "        os.path.join(Config.DATA_PATH,\"ground_truth_validation\")\n",
    "    )\n",
    "\n",
    "    print(f\"Device: {Config.DEVICE}  AMP: {Config.USE_AMP}\")\n",
    "    print(f\"Train samples: {len(train_ds)}, Val samples: {len(val_ds)}\")\n",
    "\n",
    "    train_loader = DataLoader(train_ds, batch_size=Config.BATCH_SIZE, shuffle=True,\n",
    "                              num_workers=Config.NUM_WORKERS, pin_memory=True)\n",
    "    val_loader   = DataLoader(val_ds,   batch_size=Config.BATCH_SIZE, shuffle=False,\n",
    "                              num_workers=Config.NUM_WORKERS, pin_memory=True)\n",
    "\n",
    "    encoder = SinogramEncoder().to(Config.DEVICE)\n",
    "    generator = Generator().to(Config.DEVICE)\n",
    "    discriminator = Discriminator().to(Config.DEVICE)\n",
    "\n",
    "    optG = optim.Adam(list(encoder.parameters())+list(generator.parameters()),\n",
    "                      lr=Config.LR, weight_decay=Config.WEIGHT_DECAY)\n",
    "    optD = optim.Adam(discriminator.parameters(),\n",
    "                      lr=Config.LR, weight_decay=Config.WEIGHT_DECAY)\n",
    "    scheduler = OneCycleLR(optG, max_lr=Config.MAX_LR,\n",
    "                           total_steps=Config.EPOCHS*len(train_loader), pct_start=0.1)\n",
    "\n",
    "    scaler   = GradScaler(enabled=Config.USE_AMP)\n",
    "    ssim_fn  = StructuralSimilarityIndexMeasure(data_range=1.0).to(Config.DEVICE)\n",
    "    lpips_fn = LPIPS().to(Config.DEVICE)\n",
    "    vgg_feats= vgg16(weights=VGG16_Weights.IMAGENET1K_V1).features[:16].to(Config.DEVICE).eval()\n",
    "    for p in vgg_feats.parameters(): p.requires_grad=False\n",
    "\n",
    "    history, best_ssim, patience = {\"lossD\":[],\"lossG\":[],\"ssim\":[],\"psnr\":[],\"lpips\":[]}, 0.0, 0\n",
    "\n",
    "    for epoch in range(1, Config.EPOCHS+1):\n",
    "        t0 = time.time()\n",
    "        encoder.train(); generator.train(); discriminator.train()\n",
    "        sumD, sumG, sum_ssim, sum_psnr, sum_lpips = 0.0, 0.0, 0.0, 0.0, 0.0\n",
    "        n_batches = 0\n",
    "\n",
    "        for sino, gt in tqdm(train_loader, desc=f\"Epoch {epoch}\"):\n",
    "            sino, gt = sino.to(Config.DEVICE), gt.to(Config.DEVICE)\n",
    "\n",
    "            # D step\n",
    "            with autocast(enabled=Config.USE_AMP):\n",
    "                fake = generator(encoder(sino)).detach()\n",
    "                rD, fD = discriminator(gt), discriminator(fake)\n",
    "                lossD = 0.5 * (\n",
    "                    F.binary_cross_entropy_with_logits(rD, torch.ones_like(rD)) +\n",
    "                    F.binary_cross_entropy_with_logits(fD, torch.zeros_like(fD))\n",
    "                )\n",
    "            optD.zero_grad()\n",
    "            if Config.USE_AMP:\n",
    "                scaler.scale(lossD).backward()\n",
    "                scaler.step(optD)\n",
    "            else:\n",
    "                lossD.backward()\n",
    "                optD.step()\n",
    "\n",
    "            # G step\n",
    "            with autocast(enabled=Config.USE_AMP):\n",
    "                out    = generator(encoder(sino))\n",
    "                adv    = F.binary_cross_entropy_with_logits(discriminator(out),\n",
    "                                                            torch.ones_like(rD)) * Config.L_ADV\n",
    "                perp   = F.l1_loss(\n",
    "                    vgg_feats(out.repeat(1,3,1,1)),\n",
    "                    vgg_feats(gt.repeat(1,3,1,1))\n",
    "                ) * Config.L_PERP\n",
    "                ssim_l = (1 - ssim_fn(out, gt)) * Config.L_SSIM\n",
    "                cycle  = F.l1_loss(generator(encoder(out)), gt) * Config.L_CYCLE\n",
    "                edge_l = edge_loss(out, gt) * Config.L_EDGE\n",
    "                ffl    = focal_frequency_loss(out, gt) * Config.L_FFL\n",
    "                lossG  = adv + perp + ssim_l + cycle + edge_l + ffl\n",
    "            optG.zero_grad()\n",
    "            if Config.USE_AMP:\n",
    "                scaler.scale(lossG).backward()\n",
    "                scaler.step(optG)\n",
    "                scaler.update()\n",
    "            else:\n",
    "                lossG.backward()\n",
    "                optG.step()\n",
    "            scheduler.step()\n",
    "\n",
    "            # metrics\n",
    "            with torch.no_grad():\n",
    "                ssim_val = ssim_fn(out, gt).item()\n",
    "                psnr_val = psnr(out, gt, data_range=1.0).item()\n",
    "                lpips_val = lpips_fn(out.repeat(1,3,1,1), gt.repeat(1,3,1,1)).item()\n",
    "\n",
    "            sumD += lossD.item()\n",
    "            sumG += lossG.item()\n",
    "            sum_ssim += ssim_val\n",
    "            sum_psnr += psnr_val\n",
    "            sum_lpips += lpips_val\n",
    "            n_batches += 1\n",
    "\n",
    "        avgD = sumD / n_batches\n",
    "        avgG = sumG / n_batches\n",
    "        avg_ssim = sum_ssim / n_batches\n",
    "        avg_psnr = sum_psnr / n_batches\n",
    "        avg_lpips = sum_lpips / n_batches\n",
    "        epoch_time = time.time() - t0\n",
    "\n",
    "        # early stopping\n",
    "        if avg_ssim > best_ssim:\n",
    "            best_ssim = avg_ssim\n",
    "            patience = 0\n",
    "            torch.save({\n",
    "                'encoder': encoder.state_dict(),\n",
    "                'generator': generator.state_dict(),\n",
    "                'discriminator': discriminator.state_dict(),\n",
    "            }, 'best_model.pth')\n",
    "        else:\n",
    "            patience += 1\n",
    "            if patience >= Config.PATIENCE:\n",
    "                print(\"Early stopping triggered\")\n",
    "                break\n",
    "\n",
    "        # print summary\n",
    "        print(f\"Epoch {epoch:3d} | \"\n",
    "              f\"D: {avgD:.4f}  G: {avgG:.4f}  \"\n",
    "              f\"SSIM: {avg_ssim:.4f}  PSNR: {avg_psnr:.2f}  \"\n",
    "              f\"LPIPS: {avg_lpips:.4f}  Time: {epoch_time:.2f}s\")\n",
    "\n",
    "        # record history\n",
    "        history[\"lossD\"].append(avgD)\n",
    "        history[\"lossG\"].append(avgG)\n",
    "        history[\"ssim\"].append(avg_ssim)\n",
    "        history[\"psnr\"].append(avg_psnr)\n",
    "        history[\"lpips\"].append(avg_lpips)\n",
    "\n",
    "    # Final save & plot\n",
    "    torch.save(encoder.state_dict(), os.path.join(Config.SAVE_DIR,\"final_enc.pth\"))\n",
    "    torch.save(generator.state_dict(),os.path.join(Config.SAVE_DIR,\"final_gen.pth\"))\n",
    "    torch.save(discriminator.state_dict(),os.path.join(Config.SAVE_DIR,\"final_dis.pth\"))\n",
    "\n",
    "    e = list(range(1, len(history[\"lossD\"])+1))\n",
    "    plt.figure(figsize=(12,4))\n",
    "    plt.subplot(1,2,1)\n",
    "    plt.plot(e, history[\"lossD\"], label=\"D Loss\")\n",
    "    plt.plot(e, history[\"lossG\"], label=\"G Loss\")\n",
    "    plt.legend(); plt.grid(True); plt.title(\"Train Losses\")\n",
    "\n",
    "    plt.subplot(1,2,2)\n",
    "    plt.plot(e, history[\"ssim\"], label=\"SSIM\")\n",
    "    plt.plot(e, history[\"psnr\"], label=\"PSNR\")\n",
    "    plt.legend(); plt.grid(True); plt.title(\"Val Metrics\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(Config.SAVE_DIR,\"metrics.png\"))\n",
    "    plt.show()\n",
    "\n",
    "if __name__==\"__main__\":\n",
    "    print(\"Using device:\", Config.DEVICE)\n",
    "    train_and_evaluate()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
